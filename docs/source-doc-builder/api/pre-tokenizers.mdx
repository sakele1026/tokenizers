# Pre-tokenizers

<tokenizerslangcontent>
<python>
## BertPreTokenizer[[tokenizers.pre_tokenizers.BertPreTokenizer]]

[[autodoc]] tokenizers.pre_tokenizers.BertPreTokenizer

## ByteLevel[[tokenizers.pre_tokenizers.ByteLevel]]

[[autodoc]] tokenizers.pre_tokenizers.ByteLevel

## CharDelimiterSplit[[tokenizers.pre_tokenizers.CharDelimiterSplit]]

[[autodoc]] tokenizers.pre_tokenizers.CharDelimiterSplit

## Digits[[tokenizers.pre_tokenizers.Digits]]

[[autodoc]] tokenizers.pre_tokenizers.Digits

## Metaspace[[tokenizers.pre_tokenizers.Metaspace]]

[[autodoc]] tokenizers.pre_tokenizers.Metaspace

## PreTokenizer[[tokenizers.pre_tokenizers.PreTokenizer]]

[[autodoc]] tokenizers.pre_tokenizers.PreTokenizer

## Punctuation[[tokenizers.pre_tokenizers.Punctuation]]

[[autodoc]] tokenizers.pre_tokenizers.Punctuation

## Sequence[[tokenizers.pre_tokenizers.Sequence]]

[[autodoc]] tokenizers.pre_tokenizers.Sequence

## Split[[tokenizers.pre_tokenizers.Split]]

[[autodoc]] tokenizers.pre_tokenizers.Split

## UnicodeScripts[[tokenizers.pre_tokenizers.UnicodeScripts]]

[[autodoc]] tokenizers.pre_tokenizers.UnicodeScripts

## Whitespace[[tokenizers.pre_tokenizers.Whitespace]]

[[autodoc]] tokenizers.pre_tokenizers.Whitespace

## WhitespaceSplit[[tokenizers.pre_tokenizers.WhitespaceSplit]]

[[autodoc]] tokenizers.pre_tokenizers.WhitespaceSplit
</python>
<rust>
The Rust API Reference is available directly on the [Docs.rs](https://docs.rs/tokenizers/latest/tokenizers/) website.
</rust>
<node>
The node API has not been documented yet.
</node>
</tokenizerslangcontent>